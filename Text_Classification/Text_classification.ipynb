{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Creating the json for kaggle api"
   ],
   "metadata": {
    "id": "ndrxrtGWNcN0",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The data set is a set of data from twitter that can classify the sentiment of a tweet regarding Covid,\n",
    "The data set has been manually tagged to ensure quality of the set. The set has been predivided into train and test.\n",
    "This set contains 2 files,\n",
    "1. Corona_NLP_test.csv\n",
    "2. Corona_NLP_train.csv\n",
    "\n",
    "both files consists of 6 columns,\n",
    "1. UserName\n",
    "2. ScreenName\n",
    "3. Location\n",
    "4. TweetAt\n",
    "5. OriginalTweet\n",
    "6. Sentiment\n",
    "\n",
    "Our notebook here uses a Sequential Model, RNN using GRU and Finally using Embedding.\n",
    "Using these models we have trained it to be able to predict the sentiment of a tweet related to covid"
   ],
   "metadata": {
    "id": "DsShTNzAg7Sh",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from os import mkdir\n",
    "\n",
    "!pip install kaggle\n",
    "!mkdir /root/.kaggle\n",
    "!ls -la"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "42lsvZMdEnH-",
    "outputId": "383dc64b-2f69-41a7-ce56-9f0ab48e99b4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.64.1)\n",
      "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
      "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (6.1.2)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2022.9.24)\n",
      "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
      "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
      "total 15740\n",
      "drwxr-xr-x 1 root root     4096 Nov 22 16:47 .\n",
      "drwxr-xr-x 1 root root     4096 Nov 22 16:46 ..\n",
      "drwxr-xr-x 4 root root     4096 Nov 18 14:33 .config\n",
      "-rw-r--r-- 1 root root  1002494 Sep  8  2020 Corona_NLP_test.csv\n",
      "-rw-r--r-- 1 root root 10500262 Sep  8  2020 Corona_NLP_train.csv\n",
      "-rw-r--r-- 1 root root  4595012 Nov 22 16:47 covid-19-nlp-text-classification.zip\n",
      "drwxr-xr-x 1 root root     4096 Nov 18 14:34 sample_data\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R6nsFPhe8jiJ",
    "outputId": "6901dda7-98e4-41d4-f4d9-4748c66f3e68",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ls: cannot access '.kaggle/': No such file or directory\n",
      "{\"username\": \"shararsiddiqui\", \"key\": \"0030c69a6996d6d07e9727567cf64046\"}covid-19-nlp-text-classification.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
      "Archive:  covid-19-nlp-text-classification.zip\n",
      "replace Corona_NLP_test.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: a\n",
      "error:  invalid response [a]\n",
      "replace Corona_NLP_test.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: a\n",
      "error:  invalid response [a]\n",
      "replace Corona_NLP_test.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: a\n",
      "error:  invalid response [a]\n",
      "replace Corona_NLP_test.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: a\n",
      "error:  invalid response [a]\n",
      "replace Corona_NLP_test.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: a\n",
      "error:  invalid response [a]\n",
      "replace Corona_NLP_test.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: a\n",
      "error:  invalid response [a]\n",
      "replace Corona_NLP_test.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "kag_json = {\"username\":\"shararsiddiqui\",\"key\":\"0030c69a6996d6d07e9727567cf64046\"}\n",
    "\n",
    "#serialize\n",
    "json_obj = json.dumps(kag_json)\n",
    "with open(\"/root/.kaggle/kaggle.json\", \"w\") as outfile:\n",
    "    outfile.write(json_obj)\n",
    "\n",
    "!ls .kaggle/\n",
    "!cat ~/.kaggle/kaggle.json\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "!kaggle datasets download -d datatattle/covid-19-nlp-text-classification\n",
    "!unzip covid-19-nlp-text-classification\n",
    "\n",
    "# search - https://www.kaggle.com/search?q=corona+virus+tweets+in%3Adatasets\n",
    "# dataset chosen - https://www.kaggle.com/datasets/datatattle/covid-19-nlp-text-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Call kaggle api to download the required datasets"
   ],
   "metadata": {
    "id": "ORpjLO7oNhlF",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "metadata": {
    "id": "BL7ZaihgNpoV",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sequential Model"
   ],
   "metadata": {
    "id": "zarwDbCCqvv_",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Explain what the data is supposed to help with"
   ],
   "metadata": {
    "id": "iBVo-orlzxfO",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# the data we obtained was already divided into train and test\n",
    "df_test = pd.read_csv('Corona_NLP_test.csv', header=0, usecols=[4,5], encoding='latin-1')\n",
    "df_train = pd.read_csv('Corona_NLP_train.csv', header=0, usecols=[4,5], encoding='latin-1')[:16000]\n",
    "\n",
    "df_test.groupby(['Sentiment'])['Sentiment'].count()\n",
    "df_train.groupby(['Sentiment'])['Sentiment'].count()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LiOK4Un8DeOn",
    "outputId": "88c3e87b-2718-4de2-af32-555b1f80978d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Sentiment\n",
       "Extremely Negative    2464\n",
       "Extremely Positive    2423\n",
       "Negative              4014\n",
       "Neutral               2813\n",
       "Positive              4286\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vectorize each tweet:"
   ],
   "metadata": {
    "id": "TmsmZ36qcN3o",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# set up X and Y\n",
    "num_labels = 5\n",
    "vocab_size = 25000\n",
    "batch_size = 100\n",
    "\n",
    "# fit the tokenizer on the training data\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(df_train.OriginalTweet)\n",
    "\n",
    "x_train = tokenizer.texts_to_matrix(df_train.OriginalTweet, mode='tfidf')\n",
    "x_test = tokenizer.texts_to_matrix(df_test.OriginalTweet, mode='tfidf')\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(df_train.Sentiment)\n",
    "y_train = encoder.transform(df_train.Sentiment)\n",
    "y_test = encoder.transform(df_test.Sentiment)\n",
    "\n",
    "# check shape\n",
    "print(\"train shapes:\", x_train.shape, y_train.shape)\n",
    "print(\"test shapes:\", x_test.shape, y_test.shape)\n",
    "print(\"test first five labels:\", y_test[:5])\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "y_train = to_categorical(y_train, num_classes=None)\n",
    "y_test = to_categorical(y_test, num_classes=None)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ahohi6mC0VR2",
    "outputId": "b950edc7-21d1-46d8-ab15-b9282f0b10d7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train shapes: (16000, 25000) (16000,)\n",
      "test shapes: (3798, 25000) (3798,)\n",
      "test first five labels: [0 4 1 2 3 3 4 3 0 1]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train a sequential model"
   ],
   "metadata": {
    "id": "yhCKBwpjcbtK",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "models.Sequential()\n",
    "seq_model.add(layers.Dense(24, input_dim=vocab_size, kernel_initializer='normal', activation='relu'))\n",
    "seq_model.add(layers.Dropout(.35))\n",
    "seq_model.add(layers.Dense(12, kernel_initializer='normal', activation='relu'))\n",
    "seq_model.add(layers.Dropout(.35))\n",
    "seq_model.add(layers.Dense(5, kernel_initializer='normal', activation='softmax'))\n",
    "\n",
    "\n",
    "seq_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "history = seq_model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=7,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4JXvTzFdz5Vk",
    "outputId": "de4de084-2b16-4cb7-ad9e-74c1796907fe",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/7\n",
      "144/144 [==============================] - 3s 13ms/step - loss: 1.5797 - accuracy: 0.2662 - val_loss: 1.5479 - val_accuracy: 0.2931\n",
      "Epoch 2/7\n",
      "144/144 [==============================] - 2s 12ms/step - loss: 1.4052 - accuracy: 0.3708 - val_loss: 1.2925 - val_accuracy: 0.4656\n",
      "Epoch 3/7\n",
      "144/144 [==============================] - 2s 12ms/step - loss: 1.1261 - accuracy: 0.5253 - val_loss: 1.1496 - val_accuracy: 0.5356\n",
      "Epoch 4/7\n",
      "144/144 [==============================] - 2s 11ms/step - loss: 0.9001 - accuracy: 0.6422 - val_loss: 1.1277 - val_accuracy: 0.5394\n",
      "Epoch 5/7\n",
      "144/144 [==============================] - 2s 11ms/step - loss: 0.7387 - accuracy: 0.7233 - val_loss: 1.1682 - val_accuracy: 0.5400\n",
      "Epoch 6/7\n",
      "144/144 [==============================] - 2s 12ms/step - loss: 0.5973 - accuracy: 0.7871 - val_loss: 1.2434 - val_accuracy: 0.5456\n",
      "Epoch 7/7\n",
      "144/144 [==============================] - 2s 12ms/step - loss: 0.5050 - accuracy: 0.8226 - val_loss: 1.4017 - val_accuracy: 0.5381\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# evaluate\n",
    "score = seq_model.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)\n",
    "print('Accuracy: ', score[1])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WQ72EPD89Vt7",
    "outputId": "129a2ff9-ebe4-43a5-b4a3-1c65a5879e7e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "38/38 [==============================] - 0s 5ms/step - loss: 1.4496 - accuracy: 0.5300\n",
      "Accuracy:  0.5300158262252808\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3 Try a different architecture like RNN, CNN, etc and evaluate on the test data"
   ],
   "metadata": {
    "id": "wwxY7Y_D_KL7",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## GRU (RNN) Architecture\n",
    "Here, we train a GRU without any embedding layer. Instead, we use a one-hot encoding with a vocab size of 500.\n",
    "\n",
    "Vectorize the words of each tweet:"
   ],
   "metadata": {
    "id": "PCtj2ucY_SNK",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "LSTM_VOCAB_SIZE = 500\n",
    "\n",
    "maxlen = 500\n",
    "batch_size = 32\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(df_train.Sentiment)\n",
    "y_train = encoder.transform(df_train.Sentiment)\n",
    "y_test = encoder.transform(df_test.Sentiment)\n",
    "\n",
    "encoder = keras.layers.TextVectorization(max_tokens=LSTM_VOCAB_SIZE)\n",
    "encoder.adapt(df_train.OriginalTweet)\n",
    "\n",
    "x_train = encoder(df_train.OriginalTweet)\n",
    "x_test = encoder(df_test.OriginalTweet)\n",
    "\n",
    "x_train = tf.keras.utils.to_categorical(x_train, num_classes = LSTM_VOCAB_SIZE)\n",
    "x_test = tf.keras.utils.to_categorical(x_test, num_classes = LSTM_VOCAB_SIZE)\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "y_train = to_categorical(y_train, num_classes=None)\n",
    "y_test = to_categorical(y_test, num_classes=None)\n"
   ],
   "metadata": {
    "id": "32V-W_zLGgDM",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train the gru:"
   ],
   "metadata": {
    "id": "NnSYv-NOc1T-",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "lstm = keras.Sequential()\n",
    "lstm.add(layers.Bidirectional(layers.GRU(48, input_dim=LSTM_VOCAB_SIZE)))\n",
    "lstm.add(layers.Dense(10, activation='relu'))\n",
    "lstm.add(layers.Dense(5, activation='softmax'))\n",
    "\n",
    "lstm.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "history = lstm.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=7,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)\n",
    "\n"
   ],
   "metadata": {
    "id": "2mcXtFV4_OcP",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "dce44a9a-f0dd-4a87-d900-e7987aae74fc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/7\n",
      "450/450 [==============================] - 50s 101ms/step - loss: 1.5280 - accuracy: 0.2989 - val_loss: 1.4500 - val_accuracy: 0.3519\n",
      "Epoch 2/7\n",
      "450/450 [==============================] - 45s 100ms/step - loss: 1.3640 - accuracy: 0.4021 - val_loss: 1.2525 - val_accuracy: 0.4806\n",
      "Epoch 3/7\n",
      "450/450 [==============================] - 46s 103ms/step - loss: 1.2222 - accuracy: 0.4806 - val_loss: 1.2109 - val_accuracy: 0.4994\n",
      "Epoch 4/7\n",
      "450/450 [==============================] - 47s 105ms/step - loss: 1.1795 - accuracy: 0.5063 - val_loss: 1.2539 - val_accuracy: 0.4956\n",
      "Epoch 5/7\n",
      "450/450 [==============================] - 45s 100ms/step - loss: 1.1542 - accuracy: 0.5164 - val_loss: 1.2360 - val_accuracy: 0.4863\n",
      "Epoch 6/7\n",
      "450/450 [==============================] - 46s 102ms/step - loss: 1.1291 - accuracy: 0.5282 - val_loss: 1.2316 - val_accuracy: 0.4844\n",
      "Epoch 7/7\n",
      "450/450 [==============================] - 45s 100ms/step - loss: 1.1069 - accuracy: 0.5424 - val_loss: 1.2228 - val_accuracy: 0.4969\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# evaluate\n",
    "score = lstm.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)\n",
    "print('Accuracy: ', score[1])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "szZenRHGQb4U",
    "outputId": "d400b4c3-cb9b-4f9d-b4b7-f74a7cbb7ebe",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "119/119 [==============================] - 6s 36ms/step - loss: 1.2584 - accuracy: 0.4813\n",
      "Accuracy:  0.48130595684051514\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Using an Embedding"
   ],
   "metadata": {
    "id": "sken_FU8V6ra",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we'll apply an embedding layer. The vocab size is 10000 here"
   ],
   "metadata": {
    "id": "TE9nFZDKc5em",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "LSTM_VOCAB_SIZE = 10000\n",
    "\n",
    "maxlen = 500\n",
    "batch_size = 32\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(df_train.Sentiment)\n",
    "y_train = encoder.transform(df_train.Sentiment)\n",
    "y_test = encoder.transform(df_test.Sentiment)\n",
    "\n",
    "encoder = keras.layers.TextVectorization(max_tokens=LSTM_VOCAB_SIZE)\n",
    "encoder.adapt(df_train.OriginalTweet)\n",
    "\n",
    "x_train = encoder(df_train.OriginalTweet)\n",
    "x_test = encoder(df_test.OriginalTweet)\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "y_train = to_categorical(y_train, num_classes=None)\n",
    "y_test = to_categorical(y_test, num_classes=None)\n"
   ],
   "metadata": {
    "id": "Sn5Gifa-WGvU",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "lstm = keras.Sequential()\n",
    "lstm.add(layers.Embedding(input_dim=LSTM_VOCAB_SIZE, output_dim=64))\n",
    "lstm.add(layers.Bidirectional(layers.GRU(64, input_dim=LSTM_VOCAB_SIZE)))\n",
    "lstm.add(layers.Dense(10, activation='relu'))\n",
    "lstm.add(layers.Dense(5, activation='softmax'))\n",
    "\n",
    "lstm.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "history = lstm.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=7,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mo1spn1vWOV0",
    "outputId": "0e42b1a7-57c5-40e5-ea11-9796981336f1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/7\n",
      "450/450 [==============================] - 33s 65ms/step - loss: 1.4513 - accuracy: 0.3368 - val_loss: 1.2816 - val_accuracy: 0.4412\n",
      "Epoch 2/7\n",
      "450/450 [==============================] - 27s 61ms/step - loss: 0.9409 - accuracy: 0.6203 - val_loss: 0.8421 - val_accuracy: 0.6712\n",
      "Epoch 3/7\n",
      "450/450 [==============================] - 32s 72ms/step - loss: 0.6459 - accuracy: 0.7655 - val_loss: 0.8091 - val_accuracy: 0.7063\n",
      "Epoch 4/7\n",
      "450/450 [==============================] - 27s 61ms/step - loss: 0.4968 - accuracy: 0.8285 - val_loss: 0.9122 - val_accuracy: 0.6800\n",
      "Epoch 5/7\n",
      "450/450 [==============================] - 31s 68ms/step - loss: 0.3778 - accuracy: 0.8742 - val_loss: 0.9791 - val_accuracy: 0.6787\n",
      "Epoch 6/7\n",
      "450/450 [==============================] - 27s 61ms/step - loss: 0.2821 - accuracy: 0.9119 - val_loss: 1.1017 - val_accuracy: 0.6669\n",
      "Epoch 7/7\n",
      "450/450 [==============================] - 27s 61ms/step - loss: 0.2145 - accuracy: 0.9349 - val_loss: 1.2213 - val_accuracy: 0.6712\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# evaluate\n",
    "score = lstm.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)\n",
    "print('Accuracy: ', score[1])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7x996M-KXecp",
    "outputId": "f4d71e1f-bdbd-4266-ae0d-1635064164ff",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "119/119 [==============================] - 3s 16ms/step - loss: 1.3386 - accuracy: 0.6477\n",
      "Accuracy:  0.647709310054779\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Performance Evaluation\n",
    "## Testing Accuracies\n",
    "Below you can see the testing accuracies of each model:\n",
    "\n",
    "*   Basic Sequential: `53%`\n",
    "*   RNN/Gru Architecture (without embedding): `48.1%`\n",
    "*   RNN/Gru Architecture (with embedding layer): `64.77%`\n",
    "\n",
    "NOTE: While these accuracies may seem low, it should be noted that it is a multi-category classification task. An untrained model should only get an accuracy of 1/5 = 20% on average, so these accuracies are relatively high.\n",
    "\n",
    "## Time Complexity\n",
    "The sequential model trained and evaluated quickly, while the RNN architectures were slow to evaluate and train.\n",
    "\n",
    "## Comparison\n",
    "We think that the Gru model was the best. It has the highest testing accuracy, so it is likely to generalize to the problem domain. This approach likely succeeded because it incorporated both ideas of recurrence and embedding, which are appropriate for the task of text classificaiton.\n",
    "\n",
    "## Bias / Variance Tradeoff\n",
    "When we trained the models, we noticed the bias/variance tradeoff. The more parameters we gave the model, the better the training accuracy was, but at after a certain point, increasing the size of the model actually worsens the general performance on test data.\n",
    "\n"
   ],
   "metadata": {
    "id": "AxiQwO07dOCB",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ]
}