{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# **1**\n",
    "## **WordNet**\n",
    "Wordnet is a hierarchical organization of nouns, verbs, adjectives and adverbs.\n",
    "This was developed at Princeton.\n",
    "We can use it to find synset, definitions, example and relation to other words (hypernym, hyponym, meronym, holonym, troponym etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **2**\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[Synset('manner.n.01'),\n Synset('means.n.01'),\n Synset('direction.n.01'),\n Synset('way.n.04'),\n Synset('way.n.05'),\n Synset('way.n.06'),\n Synset('way.n.07'),\n Synset('room.n.02'),\n Synset('way.n.09'),\n Synset('way.n.10'),\n Synset('way.n.11'),\n Synset('way.n.12'),\n Synset('way.r.01')]"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choosing a noun to work with\n",
    "from nltk.corpus import wordnet\n",
    "wordnet.synsets('way')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **3**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# extracting definition from one of the synsets\n",
    "wordnet.synset('direction.n.01').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# getting example\n",
    "wordnet.synset('direction.n.01').examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# getting lemma\n",
    "wordnet.synset('direction.n.01').lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# traversing the hierarchy\n",
    "word_hyp = wordnet.synset('direction.n.01')\n",
    "hyper = lambda s: s.hypernyms()\n",
    "list(word_hyp.closure(hyper))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## **Observation**\n",
    "In the output we can see that the further up the hierarchy we go, the word gets more generalized\n",
    "For example, lions->wildcats->carnivores->mammals->animals and so on.\n",
    "Which makes sense as the further up the hypernyms of a word we go we get to see more generalized word for the given word"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **4**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wordnet.synset('direction.n.01').hypernyms()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wordnet.synset('direction.n.01').hyponyms()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wordnet.synset('direction.n.01').part_meronyms()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wordnet.synset('direction.n.01').part_holonyms()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "antonyms = []\n",
    "lemmatized = wordnet.synset('direction.n.01').lemmas()\n",
    "for x in lemmatized:\n",
    "    if x.antonyms():\n",
    "        antonyms.append(x.antonyms()[0].name())\n",
    "\n",
    "print(antonyms)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **5**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "[Synset('survey.n.01'),\n Synset('study.n.02'),\n Synset('report.n.01'),\n Synset('study.n.04'),\n Synset('study.n.05'),\n Synset('discipline.n.01'),\n Synset('sketch.n.01'),\n Synset('cogitation.n.02'),\n Synset('study.n.09'),\n Synset('study.n.10'),\n Synset('analyze.v.01'),\n Synset('study.v.02'),\n Synset('study.v.03'),\n Synset('learn.v.04'),\n Synset('study.v.05'),\n Synset('study.v.06')]"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# synset of verb\n",
    "wordnet.synsets('study')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **6**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# extracting definition from one of the synsets\n",
    "wordnet.synset('analyze.v.01').definition()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# getting example\n",
    "wordnet.synset('analyze.v.01').examples()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# getting lemma\n",
    "wordnet.synset('analyze.v.01').lemmas()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# traversing the hierarchy\n",
    "word_hyp = wordnet.synset('analyze.v.01')\n",
    "hyper = lambda s: s.hypernyms()\n",
    "list(word_hyp.closure(hyper))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Observation**\n",
    "In the output we don't have any hierarchical data to make assumptions with\n",
    "Usually the hierarchy for example looks like, lions->wildcats->carnivores->mammals->animals and so on.\n",
    "From the observation we can see that not all verbs will have hypernyms"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **7**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# using morphy to find NOUN, VERB, ADJ and ADV\n",
    "wordnet.morphy('analyze', wordnet.NOUN)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wordnet.morphy('analyze', wordnet.VERB)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wordnet.morphy('analyze', wordnet.ADJ)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wordnet.morphy('analyze', wordnet.ADV)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **8**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# finding two synsets from two words to compare later\n",
    "wordnet.synsets('bus')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wordnet.synsets('ship')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# finding similarity using Wu-Palmer\n",
    "\n",
    "bus = wordnet.synset('bus.n.01')\n",
    "ship = wordnet.synset('ship.n.01')\n",
    "wordnet.wup_similarity(bus, ship)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from nltk.wsd import lesk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# applying the Lesk algorithm\n",
    "bus_sentence = word_tokenize(wordnet.synset('bus.n.01').examples()[0])\n",
    "bus_sentence_string =' '.join(bus_sentence)\n",
    "print(f'Given sentence: {bus_sentence_string}')\n",
    "print(lesk(bus_sentence, 'bus', 'n'))\n",
    "\n",
    "ship_sentence = ['they', 'booked', 'a', 'ticket', 'on', 'the', 'cruise', 'ship']\n",
    "ship_sentence_string =' '.join(ship_sentence)\n",
    "print(f'Given sentence: {ship_sentence_string}')\n",
    "print(lesk(ship_sentence, 'ship', 'n'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Observation**\n",
    "In the output for 'bus' we see that it is not the correct meaning in regard to the context, here it assumes we mean bus as a topology instead of the vehicle\n",
    "In the output for 'ship' we see that it is the correct meaning in regard to the context, where it means the ship as a noun\n",
    "From the observation we can see that the Lesk algorithm is not 100% right but is usually able to get the meaning from context given it has the information in its corpus"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **9**\n",
    "## **SentiWordNet**\n",
    "This functionality is built on top of WordNet, it can be used to do sentiment analysis.\n",
    "It gives a rating of positivity, negativity and objectivity to the given sentence.\n",
    "This can be used to assess user input and what they are trying to express by it.\n",
    "It becomes specially helpful for voice assistants and AI in general."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<affection.n.01: PosScore=0.625 NegScore=0.25>\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import sentiwordnet\n",
    "\n",
    "# 'compassion.n.01'\n",
    "# selecting an emotionally charged word\n",
    "wordnet.synsets('affection')\n",
    "affection = sentiwordnet.senti_synset('affection.n.01')\n",
    "print(affection)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polarity of \"I\" : <iodine.n.01: PosScore=0.0 NegScore=0.0>\n",
      "Polarity of \"enjoy\" : <enjoy.v.01: PosScore=0.375 NegScore=0.0>\n",
      "Polarity of \"spending\" : <spending.n.01: PosScore=0.0 NegScore=0.0>\n",
      "Polarity of \"time\" : <time.n.01: PosScore=0.0 NegScore=0.0>\n",
      "Sorry polarity of \"with\" can not be determined as there is no senti_synset for it\n",
      "Sorry polarity of \"my\" can not be determined as there is no senti_synset for it\n",
      "Polarity of \"family\" : <family.n.01: PosScore=0.0 NegScore=0.0>\n"
     ]
    }
   ],
   "source": [
    "senti_sent = \"I enjoy spending time with my family\".split()\n",
    "#senti_sent = \"I enjoy spending time family\".split()\n",
    "for word in senti_sent:\n",
    "    syn_list = list(sentiwordnet.senti_synsets(word))\n",
    "    if len(syn_list) > 0 :\n",
    "        print(f'Polarity of \\\"{word}\\\" : {syn_list[0]}')\n",
    "    else:\n",
    "        print(f'Sorry polarity of \\\"{word}\\\" can not be determined as there is no senti_synset for it')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Observation**\n",
    "In the output we can see that polarity is not available for all words as they don't exist in the senti_synset.\n",
    "Also, it may give wrong answers as it makes assumptions, here \"I\" was to define a person/myself but the function assumed it meant Iodine\n",
    "But for all emotionally charged words like \"enjoy\" we can get the polarity fairly accurately.\n",
    "As we mentioned before, this can be used in AI, voice assistants and more. If I was talking to an AI and I said this sentence to it\n",
    "then I'd expect it to reply in a positive manner. If I said \"I am feeling down\" then it should reply in an encouraging manner.\n",
    "The AI would only be able to understand they proper way to respond if it is able to use sentiment analysis well."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **10**\n",
    "# Collocation\n",
    "We have many common set of words that are highly likely to be found right next to each other.\n",
    "Some general example can be, \"social media\", \"desktop computer\", \"artificial intelligence\"\n",
    "Having this knowledge we are able to fill in what word might come next."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "United States; fellow citizens; years ago; four years; Federal\n",
      "Government; General Government; American people; Vice President; God\n",
      "bless; Chief Justice; one another; fellow Americans; Old World;\n",
      "Almighty God; Fellow citizens; Chief Magistrate; every citizen; Indian\n",
      "tribes; public debt; foreign nations\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import text4\n",
    "\n",
    "print(text4.collocations())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"years ago\" appears: 27 times\n",
      "tokens in \"text4\" with no preprocessing: 152901\n",
      "\"years\" appears: 143 times\n",
      "\"ago\" appears: 44 times\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from nltk import bigrams\n",
    "\n",
    "# Calculating mutual information of \"years ago\"\n",
    "count_bigram = 0\n",
    "bi_grams = list(bigrams(text4))\n",
    "for freq in bi_grams:\n",
    "    if freq[0] == 'years' and freq[1] == 'ago':\n",
    "        count_bigram = count_bigram + 1\n",
    "\n",
    "count_one = 0\n",
    "count_two = 0\n",
    "for freq in text4:\n",
    "    if freq == 'years':\n",
    "        count_one = count_one + 1\n",
    "    if freq == 'ago':\n",
    "        count_two = count_two + 1\n",
    "\n",
    "\n",
    "print(f'\\\"years ago\\\" appears: {count_bigram} times')\n",
    "print(f'tokens in \\\"text4\\\" with no preprocessing: {len(text4)}')\n",
    "print(f'\\\"years\\\" appears: {count_one} times')\n",
    "print(f'\\\"ago\\\" appears: {count_two} times')\n",
    "\n",
    "print(\"formula is :\")\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}